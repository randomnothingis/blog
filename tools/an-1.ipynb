{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e90b878",
   "metadata": {},
   "source": [
    "\n",
    "# Blog Content Analysis Notebook\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87ea3b",
   "metadata": {},
   "source": [
    "### Import reqs and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import boto3\n",
    "import frontmatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "AWS_PROFILE = os.getenv(\"AWS_PROFILE\")\n",
    "BLOG_CONTENT_DIR = Path(\"../src/content/blog\")\n",
    "DRAFTS_CONTENT_DIR = Path(\"../src/content/drafts\")\n",
    "EMBEDDINGS_CACHE_DIR = Path(\"./cache\")\n",
    "EMBEDDINGS_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported and directories configured\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d03535",
   "metadata": {},
   "source": [
    "## AWS setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Setup\n",
    "def initialize_aws_session(profile_name: str = \"default\", region_name: str = \"eu-central-1\"):\n",
    "    \"\"\"Initialize AWS session and verify credentials.\"\"\"\n",
    "    try:\n",
    "        session = boto3.Session(profile_name=profile_name, region_name=region_name)\n",
    "        \n",
    "        # Test credentials\n",
    "        sts_client = session.client('sts')\n",
    "        identity = sts_client.get_caller_identity()\n",
    "        \n",
    "        print(f\"✓ AWS credentials verified for profile '{profile_name}'\")\n",
    "        print(f\"  Account: {identity.get('Account')}\")\n",
    "        print(f\"  User/Role: {identity.get('Arn', '').split('/')[-1]}\")\n",
    "        print(f\"  Region: {region_name}\")\n",
    "        \n",
    "        return session\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with AWS credentials for profile '{profile_name}': {e}\")\n",
    "        print(f\"💡 Try running: aws configure --profile {profile_name}\")\n",
    "        raise\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = initialize_aws_session(AWS_PROFILE, AWS_REGION)\n",
    "bedrock_client = session.client('bedrock-runtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model names for AWS\n",
    "MODEL_SONNET_35=\"eu.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "MODEL_TITAN_V2=\"amazon.titan-embed-text-v2:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd52bd",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Functions\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean and prepare text for analysis.\"\"\"\n",
    "    # Remove markdown syntax\n",
    "    text = re.sub(r'#{1,6}\\s+', '', text)  # Headers\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'`(.*?)`', r'\\1', text)  # Code\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)  # Links\n",
    "    text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', r'\\1', text)  # Images\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_articles_from_local() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load all markdown articles from local directories.\"\"\"\n",
    "    print(\"Loading articles from local directories...\")\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    # Load from blog and drafts directories\n",
    "    for content_dir, article_type in [\n",
    "        (BLOG_CONTENT_DIR, \"blog\"), \n",
    "        # (DRAFTS_CONTENT_DIR, \"draft\")\n",
    "        ]:\n",
    "        if not content_dir.exists():\n",
    "            print(f\"Directory {content_dir} does not exist, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        md_files = list(content_dir.glob(\"*.md\")) + list(content_dir.glob(\"*.mdx\"))\n",
    "        \n",
    "        for file_path in md_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Parse frontmatter\n",
    "                post = frontmatter.loads(content)\n",
    "                \n",
    "                # Create full text for embedding (title + content)\n",
    "                title = post.metadata.get(\"title\", file_path.stem)\n",
    "                full_text = f\"{title}\\n\\n{post.content.strip()}\"\n",
    "                \n",
    "                article = {\n",
    "                    \"file_path\": str(file_path),\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"title\": title,\n",
    "                    \"date\": post.metadata.get(\"pubDate\", \"\"),\n",
    "                    \"tags\": post.metadata.get(\"tags\", []),\n",
    "                    \"type\": article_type,\n",
    "                    \"metadata\": post.metadata,\n",
    "                    \"content\": post.content.strip(),\n",
    "                    \"full_text\": full_text,\n",
    "                    \"processed_content\": preprocess_text(full_text), # preprocess text or not\n",
    "                    \"llm_tags\": post.metadata.get(\"LLM Tags\", []),\n",
    "                    \"llm_categories\": post.metadata.get(\"LLM Categories\", []),\n",
    "                    \"llm_summary\": post.metadata.get(\"Summary\", \"\")\n",
    "                }\n",
    "                articles.append(article)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"✓ Loaded {len(articles)} articles\")\n",
    "    return articles\n",
    "\n",
    "# Load articles\n",
    "articles = load_articles_from_local()\n",
    "\n",
    "# Display basic info\n",
    "df = pd.DataFrame(articles)\n",
    "print(f\"\\nArticle types: {df['type'].value_counts().to_dict()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Average content length: {df['content'].str.len().mean():.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35644f36",
   "metadata": {},
   "source": [
    "## Generate Embeddings for the whole article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Generation with Caching\n",
    "def get_content_hash(text: str) -> str:\n",
    "    \"\"\"Generate hash for content to detect changes.\"\"\"\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for a single text using AWS Bedrock.\"\"\"\n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    \n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "        body=body,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['embedding']\n",
    "\n",
    "def generate_embeddings_with_cache(articles: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings with caching to avoid re-computation.\"\"\"\n",
    "    print(\"Generating embeddings with caching...\")\n",
    "    \n",
    "    embeddings = []\n",
    "    cache_file = EMBEDDINGS_CACHE_DIR / \"embeddings_cache.pkl\"\n",
    "    \n",
    "    # Load existing cache\n",
    "    cache = {}\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "        print(f\"✓ Loaded cache with {len(cache)} embeddings\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    for i, article in enumerate(articles):\n",
    "        content_hash = get_content_hash(article[\"processed_content\"])\n",
    "        \n",
    "        if content_hash in cache:\n",
    "            embedding = cache[content_hash]\n",
    "            print(f\"  {i+1}/{len(articles)}: Using cached embedding for '{article['title'][:50]}...'\")\n",
    "        else:\n",
    "            print(f\"  {i+1}/{len(articles)}: Generating embedding for '{article['title'][:50]}...'\")\n",
    "            embedding = generate_embedding(article[\"processed_content\"])\n",
    "            cache[content_hash] = embedding\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Save updated cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "    print(f\"✓ Saved cache with {len(cache)} embeddings\")\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = generate_embeddings_with_cache(articles)\n",
    "print(f\"✓ Generated embeddings shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42c168",
   "metadata": {},
   "source": [
    "## Clustering articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d22a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Clustering with HDBSCAN\n",
    "def cluster_articles(embeddings: np.ndarray, min_cluster_size: int = 2, min_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\"Cluster articles using HDBSCAN.\"\"\"\n",
    "    print(f\"Clustering {len(embeddings)} articles...\")\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_epsilon=0.01\n",
    "    )\n",
    "    \n",
    "    labels = clusterer.fit_predict(embeddings)\n",
    "    \n",
    "    # Print cluster statistics\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    print(f\"✓ Found {n_clusters} clusters\")\n",
    "    print(f\"  Noise points (unclustered): {n_noise}\")\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1:\n",
    "            count = np.sum(labels == label)\n",
    "            print(f\"  Cluster {label}: {count} articles\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Perform clustering\n",
    "cluster_labels = cluster_articles(embeddings, min_cluster_size=2)\n",
    "\n",
    "# Add cluster labels to articles\n",
    "for i, article in enumerate(articles):\n",
    "    article['cluster'] = int(cluster_labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5acdde",
   "metadata": {},
   "source": [
    "## Visualise embeddings with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e542b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Visualization with UMAP\n",
    "def create_umap_visualization(embeddings: np.ndarray, articles: List[Dict[str, Any]], \n",
    "                            cluster_labels: np.ndarray) -> go.Figure:\n",
    "    \"\"\"Create UMAP visualization of article clusters.\"\"\"\n",
    "    print(\"Creating UMAP visualization...\")\n",
    "    \n",
    "    # Reduce dimensionality with UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    embedding_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'x': embedding_2d[:, 0],\n",
    "        'y': embedding_2d[:, 1],\n",
    "        'cluster': cluster_labels,\n",
    "        'title': [article['title'] for article in articles],\n",
    "        'type': [article['type'] for article in articles],\n",
    "        'filename': [article['filename'] for article in articles],\n",
    "        'date': [article['date'] for article in articles]\n",
    "    })\n",
    "    \n",
    "    # Create color map for clusters\n",
    "    unique_clusters = sorted(plot_df['cluster'].unique())\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    color_map = {cluster: colors[i % len(colors)] for i, cluster in enumerate(unique_clusters)}\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        cluster_data = plot_df[plot_df['cluster'] == cluster]\n",
    "        cluster_name = f\"Cluster {cluster}\" if cluster != -1 else \"Unclustered\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['x'],\n",
    "            y=cluster_data['y'],\n",
    "            mode='markers',\n",
    "            name=cluster_name,\n",
    "            marker=dict(\n",
    "                color=color_map[cluster],\n",
    "                size=8,\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            text=cluster_data['title'],\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Type: %{customdata[0]}<br>' +\n",
    "                         'Date: %{customdata[1]}<br>' +\n",
    "                         'File: %{customdata[2]}<br>' +\n",
    "                         '<extra></extra>',\n",
    "            customdata=cluster_data[['type', 'date', 'filename']].values\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Blog Articles Clustered by Content Similarity (UMAP)\",\n",
    "        xaxis_title=\"UMAP Dimension 1\",\n",
    "        yaxis_title=\"UMAP Dimension 2\",\n",
    "        width=1000,\n",
    "        height=700,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display visualization\n",
    "fig = create_umap_visualization(embeddings, articles, cluster_labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig.write_html(\"./articles-umap.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79fdb4",
   "metadata": {},
   "source": [
    "## Extracting tags and categories using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Individual Article Analysis with LLM\n",
    "def analyze_article_with_llm(article: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate topics, tags, and main points for a single article using LLM.\"\"\"\n",
    "    \n",
    "    # Prepare content for analysis\n",
    "    content_for_analysis = article['content']  # [:4000] Limit to ~4k chars \n",
    "    title = article['title']\n",
    "    \n",
    "    prompt = f\"\"\"    \n",
    "    Analyze the following article and respond in the exact JSON format below.  \n",
    "    Your task is to:  \n",
    "    1. Suggest **tags**: short keywords or phrases (3–7 items) that help with search and discovery.  \n",
    "    2. Suggest **categories**: 1–3 broader groupings under which this article should appear on a blog (e.g., topic area, type of article).  \n",
    "    3. Provide a **1-sentence summary** that clearly states what the article is about (concise and neutral).  \n",
    "\n",
    "    Do not include explanations or extra text—only return valid JSON.  \n",
    "\n",
    "    Article Title: {title}  \n",
    "\n",
    "    Article Content:  \n",
    "    {content_for_analysis}  \n",
    "\n",
    "    Respond in this exact JSON format:  \n",
    "    {{\n",
    "        \"tags\": [\"tag1\", \"tag2\", \"tag3\"],\n",
    "        \"categories\": [\"category1\", \"category2\"],\n",
    "        \"summary\": \"Brief 1-sentence summary of the article\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.0,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=MODEL_SONNET_35,\n",
    "            body=body,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        llm_response = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            analysis = json.loads(llm_response)\n",
    "            return {\n",
    "                \"llm_tags\": analysis.get(\"tags\", []),\n",
    "                \"llm_categories\": analysis.get(\"categories\", []),\n",
    "                \"llm_summary\": analysis.get(\"summary\", \"\"),\n",
    "                \"llm_analysis_success\": True\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback if JSON parsing fails\n",
    "            return {\n",
    "                \"llm_tags\": [],\n",
    "                \"llm_categories\": [],\n",
    "                \"llm_summary\": llm_response[:200] + \"...\" if len(llm_response) > 200 else llm_response,\n",
    "                \"llm_analysis_success\": False\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing article '{title}': {e}\")\n",
    "        return {\n",
    "            \"llm_tags\": [],\n",
    "            \"llm_categories\": [],\n",
    "            \"llm_summary\": \"\",\n",
    "            \"llm_analysis_success\": False\n",
    "        }\n",
    "\n",
    "def analyze_all_articles_with_llm(articles: List[Dict[str, Any]], \n",
    "                                 max_articles: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze all articles with LLM (with optional limit for testing).\"\"\"\n",
    "    print(\"Analyzing articles with LLM for topics, tags, and main points...\")\n",
    "    \n",
    "    articles_to_analyze = articles[:max_articles] if max_articles else articles\n",
    "    \n",
    "    for i, article in enumerate(articles_to_analyze):\n",
    "        print(f\"  {i+1}/{len(articles_to_analyze)}: Analyzing '{article['title'][:50]}...'\")\n",
    "        \n",
    "        analysis = analyze_article_with_llm(article)\n",
    "        print(\"analysis\", analysis)\n",
    "        \n",
    "        # Add analysis results to article\n",
    "        article.update(analysis)\n",
    "        \n",
    "        if analysis['llm_analysis_success']:\n",
    "            print(f\"    ✓ Generated {len(analysis['llm_tags'])} tags, {len(analysis['llm_categories'])} categories\")\n",
    "        else:\n",
    "            print(f\"    ⚠️  Analysis partially failed\")\n",
    "    \n",
    "    return articles_to_analyze\n",
    "\n",
    "# Analyze articles (start with first 5 for testing, remove limit to analyze all)\n",
    "analyzed_articles = analyze_all_articles_with_llm(articles, max_articles=5)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE ARTICLE ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for article in analyzed_articles[:5]:  # Show first N articles\n",
    "    print(f\"\\n📄 {article['title']}\")\n",
    "    print(f\"LLM Tags: {', '.join(article['llm_tags'])}\")\n",
    "    print(f\"LLM Categories: {', '.join(article['llm_categories'])}\")\n",
    "    print(f\"Summary: {article['llm_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823b219",
   "metadata": {},
   "source": [
    "save the tags, categories and summary for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a962764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LLM results back into frontmatter of each file\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import frontmatter\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def save_llm_to_frontmatter(article: Dict[str, Any],\n",
    "                            keys_map: Dict[str, str] = None,\n",
    "                            backup: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Write LLM fields from article dict into the file frontmatter.\n",
    "    keys_map maps article keys -> frontmatter keys (defaults below).\n",
    "    Returns True on success.\n",
    "    \"\"\"\n",
    "    if keys_map is None:\n",
    "        keys_map = {\n",
    "            \"llm_tags\": \"LLM Tags\",\n",
    "            \"llm_categories\": \"LLM Categories\",\n",
    "            \"llm_summary\": \"Summary\",\n",
    "            \"llm_main_points\": \"LLM Main Points\"\n",
    "        }\n",
    "\n",
    "    file_path = article.get(\"file_path\") or article.get(\"filename\")\n",
    "    if not file_path:\n",
    "        print(\"No file path in article, skipping\")\n",
    "        return False\n",
    "\n",
    "    fp = Path(file_path)\n",
    "    if not fp.exists():\n",
    "        print(f\"File not found, skipping: {fp}\")\n",
    "        return False\n",
    "\n",
    "    # Load existing frontmatter\n",
    "    post = frontmatter.load(fp)\n",
    "\n",
    "    # Update metadata with lists/strings from article (preserve types)\n",
    "    for a_key, fm_key in keys_map.items():\n",
    "        if a_key in article and article[a_key] is not None:\n",
    "            post.metadata[fm_key] = article[a_key]\n",
    "\n",
    "    # Optional backup (safe copy)\n",
    "    if backup:\n",
    "        bak = fp.with_suffix(fp.suffix + \".bak\")\n",
    "        try:\n",
    "            shutil.copy(fp, bak)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Write atomically to temp then replace\n",
    "    tmp = fp.with_suffix(fp.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(frontmatter.dumps(post))\n",
    "    tmp.replace(fp)\n",
    "\n",
    "    return True\n",
    "\n",
    "def save_all_llm_to_files(articles: List[Dict[str, Any]], backup: bool = True):\n",
    "    for art in articles:\n",
    "        ok = save_llm_to_frontmatter(art, backup=backup)\n",
    "        if ok:\n",
    "            print(f\"Saved LLM fields to {art.get('file_path')}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f81f1e",
   "metadata": {},
   "source": [
    "### Generate embeddings for extracted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract all tags and categories\n",
    "def extract_all_tags_and_categories(articles: List[Dict[str, Any]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Extract all unique tags and categories from analyzed articles.\"\"\"\n",
    "    all_tags = set()\n",
    "    all_categories = set()\n",
    "    \n",
    "    for article in articles:\n",
    "        if 'llm_tags' in article:\n",
    "            all_tags.update(article['llm_tags'])\n",
    "        if 'llm_categories' in article:\n",
    "            all_categories.update(article['llm_categories'])\n",
    "    \n",
    "    return {\n",
    "        'tags': list(all_tags),\n",
    "        'categories': list(all_categories)\n",
    "    }\n",
    "\n",
    "terms_data = extract_all_tags_and_categories(articles)\n",
    "\n",
    "print(f\"Extracted {len(terms_data['tags'])} unique tags\")\n",
    "print(f\"Extracted {len(terms_data['categories'])} unique categories\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ae72e",
   "metadata": {},
   "source": [
    "### Actually generate the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Generate embeddings for terms\n",
    "\n",
    "def generate_embeddings_for_terms(terms: List[str], bedrock_client) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for tags/categories.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(terms)} terms...\")\n",
    "    \n",
    "    embeddings = []\n",
    "    for term in terms:\n",
    "        try:\n",
    "            body = json.dumps({\n",
    "                \"inputText\": term\n",
    "            })\n",
    "            \n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "                body=body,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            embedding = response_body['embedding']\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for '{term}': {e}\")\n",
    "            # Use zero vector as fallback\n",
    "            embeddings.append([0.0] * 1536)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "all_terms = terms_data['tags'] + terms_data['categories']\n",
    "term_embeddings = generate_embeddings_for_terms(all_terms, bedrock_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda48bf",
   "metadata": {},
   "source": [
    "### Cluster terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe84ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Cluster tags and categories (separately)\n",
    "\n",
    "def cluster_terms(terms: List[str], embeddings: np.ndarray, min_cluster_size: int = 2) -> Dict[int, List[str]]:\n",
    "    \"\"\"Cluster terms by semantic similarity.\"\"\"\n",
    "    print(f\"Clustering {len(terms)} terms...\")\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=1,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_epsilon=0.1\n",
    "    )\n",
    "    \n",
    "    labels = clusterer.fit_predict(embeddings)\n",
    "    \n",
    "    # Group terms by cluster\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(terms[i])\n",
    "    \n",
    "    # Print cluster info\n",
    "    n_clusters = len([k for k in clusters.keys() if k != -1])\n",
    "    n_noise = len(clusters.get(-1, []))\n",
    "    \n",
    "    print(f\"✓ Found {n_clusters} term clusters\")\n",
    "    if n_noise > 0:\n",
    "        print(f\"  Unclustered terms: {n_noise}\")\n",
    "    \n",
    "    for cluster_id, cluster_terms in clusters.items():\n",
    "        if cluster_id != -1:\n",
    "            print(f\"  Cluster {cluster_id}: {cluster_terms}\")\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "tag_embeddings = term_embeddings[:len(terms_data['tags'])]\n",
    "category_embeddings = term_embeddings[len(terms_data['tags']):]\n",
    "\n",
    "tag_clusters = cluster_terms(terms_data['tags'], tag_embeddings, min_cluster_size=2)\n",
    "category_clusters = cluster_terms(terms_data['categories'], category_embeddings, min_cluster_size=2)\n",
    "\n",
    "\n",
    "\n",
    "# Generate blog taxonomy\n",
    "# taxonomy_results = create_blog_taxonomy(analyzed_articles, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38a470",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_umap_visualization(embeddings: np.ndarray, articles: List[Dict[str, Any]], \n",
    "                            cluster_labels: np.ndarray, \n",
    "                            tag_embeddings: np.ndarray = None,\n",
    "                            tag_clusters: Dict[int, List[str]] = None,\n",
    "                            all_tags: List[str] = None) -> go.Figure:\n",
    "    \"\"\"Create UMAP visualization of article clusters and optionally tag clusters.\"\"\"\n",
    "    print(\"Creating UMAP visualization...\")\n",
    "    \n",
    "    # Combine embeddings if tags are provided\n",
    "    if tag_embeddings is not None and all_tags is not None:\n",
    "        print(f\"Including {len(all_tags)} tags in visualization...\")\n",
    "        combined_embeddings = np.vstack([embeddings, tag_embeddings])\n",
    "        \n",
    "        # Create labels for combined data\n",
    "        item_types = ['Article'] * len(articles) + ['Tag'] * len(all_tags)\n",
    "        item_names = [article['title'] for article in articles] + [f\"Tag: {tag}\" for tag in all_tags]\n",
    "        \n",
    "        # Create cluster labels for combined data\n",
    "        if tag_clusters is not None:\n",
    "            # Map tags to their cluster labels\n",
    "            tag_cluster_labels = []\n",
    "            for tag in all_tags:\n",
    "                # Find which cluster this tag belongs to\n",
    "                tag_cluster = -1  # Default to noise\n",
    "                for cluster_id, cluster_tags in tag_clusters.items():\n",
    "                    if tag in cluster_tags:\n",
    "                        tag_cluster = cluster_id + 100  # Offset tag clusters to avoid confusion with article clusters\n",
    "                        break\n",
    "                tag_cluster_labels.append(tag_cluster)\n",
    "            \n",
    "            combined_cluster_labels = np.concatenate([cluster_labels, tag_cluster_labels])\n",
    "        else:\n",
    "            # All tags get a special cluster label\n",
    "            combined_cluster_labels = np.concatenate([cluster_labels, [-10] * len(all_tags)])\n",
    "    else:\n",
    "        combined_embeddings = embeddings\n",
    "        item_types = ['Article'] * len(articles)\n",
    "        item_names = [article['title'] for article in articles]\n",
    "        combined_cluster_labels = cluster_labels\n",
    "    \n",
    "    # Reduce dimensionality with UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    embedding_2d = reducer.fit_transform(combined_embeddings)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = {\n",
    "        'x': embedding_2d[:, 0],\n",
    "        'y': embedding_2d[:, 1],\n",
    "        'cluster': combined_cluster_labels,\n",
    "        'type': item_types,\n",
    "        'name': item_names\n",
    "    }\n",
    "    \n",
    "    # Add article-specific data\n",
    "    if tag_embeddings is not None:\n",
    "        plot_data['title'] = [article['title'] for article in articles] + [''] * len(all_tags)\n",
    "        plot_data['article_type'] = [article['type'] for article in articles] + [''] * len(all_tags)\n",
    "        plot_data['filename'] = [article['filename'] for article in articles] + [''] * len(all_tags)\n",
    "        plot_data['date'] = [article['date'] for article in articles] + [''] * len(all_tags)\n",
    "    else:\n",
    "        plot_data['title'] = [article['title'] for article in articles]\n",
    "        plot_data['article_type'] = [article['type'] for article in articles]\n",
    "        plot_data['filename'] = [article['filename'] for article in articles]\n",
    "        plot_data['date'] = [article['date'] for article in articles]\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create color map for clusters\n",
    "    unique_clusters = sorted(plot_df['cluster'].unique())\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    color_map = {cluster: colors[i % len(colors)] for i, cluster in enumerate(unique_clusters)}\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot articles\n",
    "    article_data = plot_df[plot_df['type'] == 'Article']\n",
    "    for cluster in sorted(article_data['cluster'].unique()):\n",
    "        cluster_data = article_data[article_data['cluster'] == cluster]\n",
    "        cluster_name = f\"Article Cluster {cluster}\" if cluster != -1 else \"Unclustered Articles\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['x'],\n",
    "            y=cluster_data['y'],\n",
    "            mode='markers+text',\n",
    "            name=cluster_name,\n",
    "            marker=dict(\n",
    "                color=color_map[cluster],\n",
    "                size=8,\n",
    "                opacity=0.7,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            text=cluster_data['title'],\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Type: Article<br>' +\n",
    "                         'Article Type: %{customdata[0]}<br>' +\n",
    "                         'Date: %{customdata[1]}<br>' +\n",
    "                         'File: %{customdata[2]}<br>' +\n",
    "                         '<extra></extra>',\n",
    "            customdata=cluster_data[['article_type', 'date', 'filename']].values,\n",
    "            legendgroup='articles'\n",
    "        ))\n",
    "    \n",
    "    # Plot tags if provided\n",
    "    if tag_embeddings is not None:\n",
    "        tag_data = plot_df[plot_df['type'] == 'Tag']\n",
    "        \n",
    "        if tag_clusters is not None:\n",
    "            # Plot tags by cluster\n",
    "            for cluster in sorted(tag_data['cluster'].unique()):\n",
    "                cluster_data = tag_data[tag_data['cluster'] == cluster]\n",
    "                \n",
    "                if cluster == -10:\n",
    "                    cluster_name = \"Unclustered Tags\"\n",
    "                    marker_color = 'orange'\n",
    "                elif cluster >= 100:\n",
    "                    actual_cluster = cluster - 100\n",
    "                    cluster_name = f\"Tag Cluster {actual_cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'orange')\n",
    "                else:\n",
    "                    cluster_name = f\"Tag Cluster {cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'orange')\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=cluster_data['x'],\n",
    "                    y=cluster_data['y'],\n",
    "                    mode='markers',\n",
    "                    name=cluster_name,\n",
    "                    marker=dict(\n",
    "                        color=marker_color,\n",
    "                        size=12,\n",
    "                        opacity=0.8,\n",
    "                        symbol='diamond',\n",
    "                        line=dict(width=2, color='white')\n",
    "                    ),\n",
    "                    text=cluster_data['name'],\n",
    "                    hovertemplate='<b>%{text}</b><br>' +\n",
    "                                 'Type: Tag<br>' +\n",
    "                                 'Cluster: ' + cluster_name + '<br>' +\n",
    "                                 '<extra></extra>',\n",
    "                    legendgroup='tags'\n",
    "                ))\n",
    "        else:\n",
    "            # Plot all tags with same style\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=tag_data['x'],\n",
    "                y=tag_data['y'],\n",
    "                mode='markers+text',\n",
    "                name=\"Tags\",\n",
    "                marker=dict(\n",
    "                    color='orange',\n",
    "                    size=12,\n",
    "                    opacity=0.8,\n",
    "                    symbol='diamond',\n",
    "                    line=dict(width=2, color='white')\n",
    "                ),\n",
    "                text=tag_data['name'],\n",
    "                hovertemplate='<b>%{text}</b><br>' +\n",
    "                             'Type: Tag<br>' +\n",
    "                             '<extra></extra>',\n",
    "                legendgroup='tags'\n",
    "            ))\n",
    "    \n",
    "    title = \"Blog Articles\"\n",
    "    if tag_embeddings is not None:\n",
    "        title += \" and Tags\"\n",
    "    title += \" Clustered by Content Similarity (UMAP)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"UMAP Dimension 1\",\n",
    "        yaxis_title=\"UMAP Dimension 2\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        margin=dict(r=150)  # Make room for legend\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display visualization with tags\n",
    "fig = create_umap_visualization(\n",
    "    embeddings, \n",
    "    articles, \n",
    "    cluster_labels,\n",
    "    tag_embeddings=tag_embeddings,\n",
    "    tag_clusters=tag_clusters,\n",
    "    all_tags=terms_data['tags']\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ba06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig.write_html(\"./tags-clustered.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2404138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_umap_visualization(embeddings: np.ndarray, articles: List[Dict[str, Any]], \n",
    "                            cluster_labels: np.ndarray, \n",
    "                            tag_embeddings: np.ndarray = None,\n",
    "                            tag_clusters: Dict[int, List[str]] = None,\n",
    "                            all_tags: List[str] = None,\n",
    "                            category_embeddings: np.ndarray = None,\n",
    "                            category_clusters: Dict[int, List[str]] = None,\n",
    "                            all_categories: List[str] = None) -> go.Figure:\n",
    "    \"\"\"Create UMAP visualization of article clusters and optionally tag and category clusters.\"\"\"\n",
    "    print(\"Creating UMAP visualization...\")\n",
    "    \n",
    "    # Start with articles\n",
    "    combined_embeddings = [embeddings]\n",
    "    item_types = ['Article'] * len(articles)\n",
    "    item_names = [article['title'] for article in articles]\n",
    "    combined_cluster_labels = cluster_labels.tolist()\n",
    "    \n",
    "    # Add tag embeddings if provided\n",
    "    if tag_embeddings is not None and all_tags is not None:\n",
    "        print(f\"Including {len(all_tags)} tags in visualization...\")\n",
    "        combined_embeddings.append(tag_embeddings)\n",
    "        item_types.extend(['Tag'] * len(all_tags))\n",
    "        item_names.extend([f\"Tag: {tag}\" for tag in all_tags])\n",
    "        \n",
    "        # Create cluster labels for tags\n",
    "        if tag_clusters is not None:\n",
    "            tag_cluster_labels = []\n",
    "            for tag in all_tags:\n",
    "                tag_cluster = -1  # Default to noise\n",
    "                for cluster_id, cluster_tags in tag_clusters.items():\n",
    "                    if tag in cluster_tags:\n",
    "                        tag_cluster = cluster_id + 100  # Offset tag clusters\n",
    "                        break\n",
    "                tag_cluster_labels.append(tag_cluster)\n",
    "            combined_cluster_labels.extend(tag_cluster_labels)\n",
    "        else:\n",
    "            combined_cluster_labels.extend([-10] * len(all_tags))\n",
    "    \n",
    "    # Add category embeddings if provided\n",
    "    if category_embeddings is not None and all_categories is not None:\n",
    "        print(f\"Including {len(all_categories)} categories in visualization...\")\n",
    "        combined_embeddings.append(category_embeddings)\n",
    "        item_types.extend(['Category'] * len(all_categories))\n",
    "        item_names.extend([f\"Category: {cat}\" for cat in all_categories])\n",
    "        \n",
    "        # Create cluster labels for categories\n",
    "        if category_clusters is not None:\n",
    "            category_cluster_labels = []\n",
    "            for category in all_categories:\n",
    "                category_cluster = -1  # Default to noise\n",
    "                for cluster_id, cluster_categories in category_clusters.items():\n",
    "                    if category in cluster_categories:\n",
    "                        category_cluster = cluster_id + 200  # Offset category clusters\n",
    "                        break\n",
    "                category_cluster_labels.append(category_cluster)\n",
    "            combined_cluster_labels.extend(category_cluster_labels)\n",
    "        else:\n",
    "            combined_cluster_labels.extend([-20] * len(all_categories))\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    all_embeddings = np.vstack(combined_embeddings)\n",
    "    combined_cluster_labels = np.array(combined_cluster_labels)\n",
    "    \n",
    "    print(f\"Combined embeddings shape: {all_embeddings.shape}\")\n",
    "    print(f\"Articles: {len(articles)}, Tags: {len(all_tags) if all_tags else 0}, Categories: {len(all_categories) if all_categories else 0}\")\n",
    "    \n",
    "    # Reduce dimensionality with UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    embedding_2d = reducer.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = {\n",
    "        'x': embedding_2d[:, 0],\n",
    "        'y': embedding_2d[:, 1],\n",
    "        'cluster': combined_cluster_labels,\n",
    "        'type': item_types,\n",
    "        'name': item_names\n",
    "    }\n",
    "    \n",
    "    # Add article-specific data (pad with empty strings for tags/categories)\n",
    "    n_tags = len(all_tags) if all_tags else 0\n",
    "    n_categories = len(all_categories) if all_categories else 0\n",
    "    n_total = len(articles) + n_tags + n_categories\n",
    "    \n",
    "    plot_data['title'] = [article['title'] for article in articles] + [''] * (n_tags + n_categories)\n",
    "    plot_data['article_type'] = [article['type'] for article in articles] + [''] * (n_tags + n_categories)\n",
    "    plot_data['filename'] = [article['filename'] for article in articles] + [''] * (n_tags + n_categories)\n",
    "    plot_data['date'] = [article['date'] for article in articles] + [''] * (n_tags + n_categories)\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create color map for clusters\n",
    "    unique_clusters = sorted(plot_df['cluster'].unique())\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    color_map = {cluster: colors[i % len(colors)] for i, cluster in enumerate(unique_clusters)}\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot articles\n",
    "    article_data = plot_df[plot_df['type'] == 'Article']\n",
    "    for cluster in sorted(article_data['cluster'].unique()):\n",
    "        cluster_data = article_data[article_data['cluster'] == cluster]\n",
    "        cluster_name = f\"Article Cluster {cluster}\" if cluster != -1 else \"Unclustered Articles\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['x'],\n",
    "            y=cluster_data['y'],\n",
    "            mode='markers',\n",
    "            name=cluster_name,\n",
    "            marker=dict(\n",
    "                color=color_map[cluster],\n",
    "                size=8,\n",
    "                opacity=0.7,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            text=cluster_data['title'],\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Type: Article<br>' +\n",
    "                         'Article Type: %{customdata[0]}<br>' +\n",
    "                         'Date: %{customdata[1]}<br>' +\n",
    "                         'File: %{customdata[2]}<br>' +\n",
    "                         '<extra></extra>',\n",
    "            customdata=cluster_data[['article_type', 'date', 'filename']].values,\n",
    "            legendgroup='articles'\n",
    "        ))\n",
    "    \n",
    "    # Plot tags if provided\n",
    "    if tag_embeddings is not None:\n",
    "        tag_data = plot_df[plot_df['type'] == 'Tag']\n",
    "        \n",
    "        if tag_clusters is not None:\n",
    "            # Plot tags by cluster\n",
    "            for cluster in sorted(tag_data['cluster'].unique()):\n",
    "                cluster_data = tag_data[tag_data['cluster'] == cluster]\n",
    "                \n",
    "                if cluster == -10:\n",
    "                    cluster_name = \"Unclustered Tags\"\n",
    "                    marker_color = 'orange'\n",
    "                elif cluster >= 100:\n",
    "                    actual_cluster = cluster - 100\n",
    "                    cluster_name = f\"Tag Cluster {actual_cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'orange')\n",
    "                else:\n",
    "                    cluster_name = f\"Tag Cluster {cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'orange')\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=cluster_data['x'],\n",
    "                    y=cluster_data['y'],\n",
    "                    mode='markers',\n",
    "                    name=cluster_name,\n",
    "                    marker=dict(\n",
    "                        color=marker_color,\n",
    "                        size=12,\n",
    "                        opacity=0.8,\n",
    "                        symbol='diamond',\n",
    "                        line=dict(width=2, color='white')\n",
    "                    ),\n",
    "                    text=cluster_data['name'],\n",
    "                    hovertemplate='<b>%{text}</b><br>' +\n",
    "                                 'Type: Tag<br>' +\n",
    "                                 'Cluster: ' + cluster_name + '<br>' +\n",
    "                                 '<extra></extra>',\n",
    "                    legendgroup='tags'\n",
    "                ))\n",
    "        else:\n",
    "            # Plot all tags with same style\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=tag_data['x'],\n",
    "                y=tag_data['y'],\n",
    "                mode='markers',\n",
    "                name=\"Tags\",\n",
    "                marker=dict(\n",
    "                    color='orange',\n",
    "                    size=12,\n",
    "                    opacity=0.8,\n",
    "                    symbol='diamond',\n",
    "                    line=dict(width=2, color='white')\n",
    "                ),\n",
    "                text=tag_data['name'],\n",
    "                hovertemplate='<b>%{text}</b><br>' +\n",
    "                             'Type: Tag<br>' +\n",
    "                             '<extra></extra>',\n",
    "                legendgroup='tags'\n",
    "            ))\n",
    "    \n",
    "    # Plot categories if provided\n",
    "    if category_embeddings is not None:\n",
    "        category_data = plot_df[plot_df['type'] == 'Category']\n",
    "        \n",
    "        if category_clusters is not None:\n",
    "            # Plot categories by cluster\n",
    "            for cluster in sorted(category_data['cluster'].unique()):\n",
    "                cluster_data = category_data[category_data['cluster'] == cluster]\n",
    "                \n",
    "                if cluster == -20:\n",
    "                    cluster_name = \"Unclustered Categories\"\n",
    "                    marker_color = 'red'\n",
    "                elif cluster >= 200:\n",
    "                    actual_cluster = cluster - 200\n",
    "                    cluster_name = f\"Category Cluster {actual_cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'red')\n",
    "                else:\n",
    "                    cluster_name = f\"Category Cluster {cluster}\"\n",
    "                    marker_color = color_map.get(cluster, 'red')\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=cluster_data['x'],\n",
    "                    y=cluster_data['y'],\n",
    "                    mode='markers',\n",
    "                    name=cluster_name,\n",
    "                    marker=dict(\n",
    "                        color=marker_color,\n",
    "                        size=15,\n",
    "                        opacity=0.9,\n",
    "                        symbol='star',\n",
    "                        line=dict(width=2, color='white')\n",
    "                    ),\n",
    "                    text=cluster_data['name'],\n",
    "                    hovertemplate='<b>%{text}</b><br>' +\n",
    "                                 'Type: Category<br>' +\n",
    "                                 'Cluster: ' + cluster_name + '<br>' +\n",
    "                                 '<extra></extra>',\n",
    "                    legendgroup='categories'\n",
    "                ))\n",
    "        else:\n",
    "            # Plot all categories with same style\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=category_data['x'],\n",
    "                y=category_data['y'],\n",
    "                mode='markers',\n",
    "                name=\"Categories\",\n",
    "                marker=dict(\n",
    "                    color='red',\n",
    "                    size=15,\n",
    "                    opacity=0.9,\n",
    "                    symbol='star',\n",
    "                    line=dict(width=2, color='white')\n",
    "                ),\n",
    "                text=category_data['name'],\n",
    "                hovertemplate='<b>%{text}</b><br>' +\n",
    "                             'Type: Category<br>' +\n",
    "                             '<extra></extra>',\n",
    "                legendgroup='categories'\n",
    "            ))\n",
    "    \n",
    "    # Create dynamic title\n",
    "    title_parts = [\"Blog Articles\"]\n",
    "    if tag_embeddings is not None:\n",
    "        title_parts.append(\"Tags\")\n",
    "    if category_embeddings is not None:\n",
    "        title_parts.append(\"Categories\")\n",
    "    \n",
    "    title = \", \".join(title_parts) + \" - Semantic Space Visualization (UMAP)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"UMAP Dimension 1\",\n",
    "        yaxis_title=\"UMAP Dimension 2\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        margin=dict(r=150)  # Make room for legend\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display visualization with tags and categories\n",
    "fig = create_umap_visualization(\n",
    "    embeddings, \n",
    "    articles, \n",
    "    cluster_labels,\n",
    "    tag_embeddings=tag_embeddings,\n",
    "    tag_clusters=tag_clusters,\n",
    "    all_tags=terms_data['tags'],\n",
    "    category_embeddings=category_embeddings,\n",
    "    category_clusters=category_clusters,\n",
    "    all_categories=terms_data['categories']\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"article_tags_categories_umap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb83571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blog_taxonomy_with_llm(category_clusters: Dict[int, List[str]], tag_clusters: Dict[int, List[str]],\n",
    "                                   bedrock_client) -> Dict[str, Any]:\n",
    "    \"\"\"Generate final blog taxonomy using LLM to consolidate clusters.\"\"\"\n",
    "    print(\"Generating final blog taxonomy with LLM...\")\n",
    "    \n",
    "    \n",
    "    category_cluster_summaries = []\n",
    "    for cluster_id, terms in category_clusters.items():\n",
    "        if cluster_id != -1:  # Skip noise\n",
    "            category_cluster_summaries.append(f\"Category Cluster {cluster_id}: {', '.join(terms)}\")\n",
    "    \n",
    "\n",
    "    if -1 in category_clusters:\n",
    "        category_cluster_summaries.append(f\"Unclustered Categories: {', '.join(category_clusters[-1])}\")\n",
    "\n",
    "    tag_cluster_summaries = []\n",
    "    for cluster_id, terms in tag_clusters.items():\n",
    "        if cluster_id != -1:  # Skip noise\n",
    "            tag_cluster_summaries.append(f\"Tag Cluster {cluster_id}: {', '.join(terms)}\")\n",
    "    \n",
    "    if -1 in tag_clusters:\n",
    "        tag_cluster_summaries.append(f\"Unclustered Tags: {', '.join(tag_clusters[-1])}\")\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Based on the clustered tags and categories from blog articles below, create a clear, hierarchical blog taxonomy.  \n",
    "        Your task is to:  \n",
    "        1. A consolidated list of 1-7 main blog sections/categories.\n",
    "        2. For each section, suggest 3-8 relevant tags that are well-organised and non-duplicative.\n",
    "        3. Merge similar/duplicate terms and use clear, consistent naming.\n",
    "        4. Provide a short explanation of major consolidation or naming decisions. \n",
    "\n",
    "        CATEGORY CLUSTERS:\n",
    "        {chr(10).join(category_cluster_summaries)}\n",
    "\n",
    "        TAG CLUSTERS:\n",
    "        {chr(10).join(tag_cluster_summaries)}\n",
    "\n",
    "\n",
    "        Respond in **valid JSON only** and in this exact format (no extra text):  \n",
    "        {{\n",
    "        \"main_sections\": [\n",
    "            {{\n",
    "            \"name\": \"Section Name\",\n",
    "            \"description\": \"Brief description of this section\",\n",
    "            \"tags\": [\"tag1\", \"tag2\", \"tag3\"]\n",
    "            }}\n",
    "        ],\n",
    "        \"all_tags\": [\"consolidated_tag1\", \"consolidated_tag2\"],\n",
    "        \"taxonomy_notes\": \"Short explanation of how duplicates were merged or terms renamed\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=\"eu.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            body=body,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        llm_response = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        taxonomy = json.loads(llm_response)\n",
    "        \n",
    "        print(\"✓ Generated blog taxonomy\")\n",
    "        print(f\"  Main sections: {len(taxonomy['main_sections'])}\")\n",
    "        print(f\"  Total consolidated tags: {len(taxonomy['all_tags'])}\")\n",
    "        \n",
    "        return taxonomy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating taxonomy: {e}\")\n",
    "        return {\n",
    "            \"main_sections\": [],\n",
    "            \"all_tags\": [],\n",
    "            \"taxonomy_notes\": f\"Error: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2abf13",
   "metadata": {},
   "source": [
    "## Generate Taxonomy with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b91bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_results = generate_blog_taxonomy_with_llm(category_clusters, tag_clusters, bedrock_client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (taxonomy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b951d0",
   "metadata": {},
   "source": [
    "## Save Taxonomy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_taxonomy_results(taxonomy_results: Dict[str, Any], output_file: str):\n",
    "    \"\"\"Save taxonomy results to a JSON file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(taxonomy_results, f, indent=1)\n",
    "\n",
    "save_taxonomy_results(taxonomy_results, 'taxonomy_results_1.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97741bea",
   "metadata": {},
   "source": [
    "## Use taxonomy to map articles to sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "\n",
    "def load_taxonomy(taxonomy_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load taxonomy from JSON file.\"\"\"\n",
    "    with open(taxonomy_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding_with_cache(text: str, bedrock_client) -> List[float]:\n",
    "    \"\"\"Retrieve or generate an embedding\"\"\"\n",
    "    print(f\"Generating embedding for '{text[:50]}...'\")\n",
    "    embedding = []\n",
    "    cache_file = EMBEDDINGS_CACHE_DIR / \"embeddings_cache.pkl\"\n",
    "\n",
    "    # Load existing cache\n",
    "    cache = {}\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "        print(f\"✓ Loaded cache with {len(cache)} embeddings\")\n",
    "\n",
    "    content_hash = get_content_hash(text)\n",
    "    if content_hash in cache:\n",
    "        embedding = cache[content_hash]\n",
    "        print(f\"  Using cached embedding for '{text[:50]}...'\")\n",
    "    else:\n",
    "        print(f\"  Generating embedding for '{text[:50]}...'\")\n",
    "        embedding = generate_embedding(text)\n",
    "        cache[content_hash] = embedding\n",
    "\n",
    "    # Save updated cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "    print(f\"✓ Saved cache with {len(cache)} embeddings\")\n",
    "    \n",
    "    return np.array(embedding)\n",
    "\n",
    "\n",
    "\n",
    "def map_articles_to_taxonomy_by_embeddings(\n",
    "    articles: List[Dict[str, Any]], \n",
    "    article_embeddings: np.ndarray,\n",
    "    taxonomy: Dict[str, Any],\n",
    "    bedrock_client,\n",
    "    threshold: float = 0.7\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Map articles to taxonomy sections using embedding similarity.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "        article_embeddings: Article embeddings array\n",
    "        taxonomy: Loaded taxonomy dictionary\n",
    "        bedrock_client: AWS Bedrock client for generating embeddings\n",
    "        threshold: Minimum cosine similarity to assign to a section\n",
    "    \n",
    "    Returns:\n",
    "        Articles with added 'taxonomy_section' and 'embedding_similarity' fields\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate embeddings for each taxonomy section\n",
    "    section_embeddings = {}\n",
    "    section_texts = {}\n",
    "    \n",
    "    for section in taxonomy['main_sections']:\n",
    "        section_name = section['name']\n",
    "        # Combine section name, description, and tags into text\n",
    "        section_text = f\"{section_name}. {section['description']}. Tags: {', '.join(section['tags'])}\"\n",
    "        section_texts[section_name] = section_text\n",
    "        \n",
    "        # Generate embedding\n",
    "        section_embedding = get_embedding_with_cache(section_text, bedrock_client)\n",
    "        section_embeddings[section_name] = section_embedding\n",
    "    \n",
    "    # Convert to matrix for efficient computation\n",
    "    section_names = list(section_embeddings.keys())\n",
    "    section_embedding_matrix = np.array([section_embeddings[name] for name in section_names])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(article_embeddings, section_embedding_matrix)\n",
    "    \n",
    "    # Map articles\n",
    "    updated_articles = []\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        article_copy = article.copy()\n",
    "        article_similarities = similarities[i]\n",
    "        \n",
    "        # Find best matching section\n",
    "        best_section_idx = np.argmax(article_similarities)\n",
    "        best_similarity = article_similarities[best_section_idx]\n",
    "        best_section = section_names[best_section_idx]\n",
    "        \n",
    "        if best_similarity >= threshold:\n",
    "            article_copy['taxonomy_section'] = best_section\n",
    "            article_copy['embedding_similarity'] = float(best_similarity)\n",
    "            \n",
    "            # Add similarity scores for all sections\n",
    "            similarity_scores = {\n",
    "                section_names[j]: float(article_similarities[j]) \n",
    "                for j in range(len(section_names))\n",
    "            }\n",
    "            article_copy['all_section_similarities'] = similarity_scores\n",
    "        else:\n",
    "            article_copy['taxonomy_section'] = 'Unassigned'\n",
    "            article_copy['embedding_similarity'] = float(best_similarity)\n",
    "            article_copy['all_section_similarities'] = {}\n",
    "        \n",
    "        updated_articles.append(article_copy)\n",
    "    \n",
    "    return updated_articles\n",
    "\n",
    "\n",
    "def analyze_taxonomy_mapping_results(mapped_articles: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze the results of taxonomy mapping.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'total_articles': len(mapped_articles),\n",
    "        'assigned_articles': len([a for a in mapped_articles if a['taxonomy_section'] != 'Unassigned']),\n",
    "        'unassigned_articles': len([a for a in mapped_articles if a['taxonomy_section'] == 'Unassigned']),\n",
    "        'section_distribution': Counter([a['taxonomy_section'] for a in mapped_articles]),\n",
    "        'average_scores': {},\n",
    "        'low_confidence_articles': []\n",
    "    }\n",
    "    \n",
    "    # Calculate average scores per section\n",
    "    section_scores = defaultdict(list)\n",
    "    for article in mapped_articles:\n",
    "        if article['taxonomy_section'] != 'Unassigned':\n",
    "            score_key = 'combined_score' if 'combined_score' in article else 'tag_match_score'\n",
    "            if score_key in article:\n",
    "                section_scores[article['taxonomy_section']].append(article[score_key])\n",
    "    \n",
    "    for section, scores in section_scores.items():\n",
    "        results['average_scores'][section] = np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # Find low confidence articles (might need manual review)\n",
    "    for article in mapped_articles:\n",
    "        score_key = 'combined_score' if 'combined_score' in article else 'tag_match_score'\n",
    "        if score_key in article and article[score_key] < 0.6:\n",
    "            results['low_confidence_articles'].append({\n",
    "                'title': article['title'],\n",
    "                'section': article['taxonomy_section'],\n",
    "                'score': article[score_key],\n",
    "                'tags': article.get('tags', [])\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_mapped_articles(mapped_articles: List[Dict[str, Any]], output_file: str):\n",
    "    \"\"\"Save mapped articles to JSON file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(mapped_articles, f, indent=2, default=str)\n",
    "    print(f\"✓ Mapped articles saved to {output_file}\")\n",
    "\n",
    "def print_mapping_summary(results: Dict[str, Any]):\n",
    "    \"\"\"Print a summary of the mapping results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TAXONOMY MAPPING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n📊 OVERVIEW:\")\n",
    "    print(f\"  Total articles: {results['total_articles']}\")\n",
    "    print(f\"  Successfully assigned: {results['assigned_articles']} ({results['assigned_articles']/results['total_articles']*100:.1f}%)\")\n",
    "    print(f\"  Unassigned: {results['unassigned_articles']} ({results['unassigned_articles']/results['total_articles']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🏷️  SECTION DISTRIBUTION:\")\n",
    "    for section, count in results['section_distribution'].items():\n",
    "        avg_score = results['average_scores'].get(section, 0.0)\n",
    "        print(f\"  {section}: {count} articles (avg score: {avg_score:.3f})\")\n",
    "    \n",
    "    if results['low_confidence_articles']:\n",
    "        print(f\"\\n⚠️  LOW CONFIDENCE ASSIGNMENTS ({len(results['low_confidence_articles'])} articles):\")\n",
    "        for article in results['low_confidence_articles'][:5]:  # Show first 5\n",
    "            print(f\"  - {article['title'][:50]}... → {article['section']} (score: {article['score']:.3f})\")\n",
    "        if len(results['low_confidence_articles']) > 5:\n",
    "            print(f\"  ... and {len(results['low_confidence_articles']) - 5} more\")\n",
    "\n",
    "\n",
    "# Load taxonomy\n",
    "taxonomy = load_taxonomy(\"taxonomy_results_1.json\")\n",
    "print(f\"✓ Loaded taxonomy with {len(taxonomy['main_sections'])} sections\")\n",
    "\n",
    "\n",
    "mapped_articles = map_articles_to_taxonomy_by_embeddings(\n",
    "    articles, embeddings, taxonomy, bedrock_client)\n",
    "\n",
    "# Analyze results\n",
    "results = analyze_taxonomy_mapping_results(mapped_articles)\n",
    "\n",
    "# Print summary\n",
    "print_mapping_summary(results)\n",
    "\n",
    "# Save results\n",
    "# save_mapped_articles(mapped_articles, output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db421c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map articles to taxonomy sections using their llm_tags and llm_categories, with embeddings\n",
    "\n",
    "def map_articles_to_taxonomy_by_tags_categories(\n",
    "    articles: List[Dict[str, Any]],\n",
    "    taxonomy: Dict[str, Any],\n",
    "    bedrock_client,\n",
    "    threshold: float = 0.7\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Map articles to taxonomy sections using embedding similarity of llm_tags and llm_categories.\n",
    "    Args:\n",
    "        articles: List of article dicts with 'llm_tags' and 'llm_categories'\n",
    "        taxonomy: Taxonomy dict loaded from JSON\n",
    "        bedrock_client: AWS Bedrock client\n",
    "        threshold: Minimum cosine similarity to assign to a section\n",
    "    Returns:\n",
    "        Articles with added 'taxonomy_section' and 'embedding_similarity' fields\n",
    "    \"\"\"\n",
    "    section_embeddings = {}\n",
    "    section_names = []\n",
    "    # Prepare section text for embedding\n",
    "    for section in taxonomy['main_sections']:\n",
    "        section_name = section['name']\n",
    "        section_text = f\"{section_name}. {section['description']}. Tags: {', '.join(section['tags'])}\"\n",
    "        section_embedding = get_embedding_with_cache(section_text, bedrock_client)\n",
    "        section_embeddings[section_name] = section_embedding\n",
    "        section_names.append(section_name)\n",
    "    section_embedding_matrix = np.array([section_embeddings[name] for name in section_names])\n",
    "\n",
    "    updated_articles = []\n",
    "    for article in articles:\n",
    "        # Combine llm_tags and llm_categories for embedding\n",
    "        tags = article.get('llm_tags', [])\n",
    "        categories = article.get('llm_categories', [])\n",
    "        content = article\n",
    "        combined_text = f\"{' '.join(categories)} {' '.join(tags)}\"\n",
    "        if not combined_text.strip():\n",
    "            combined_text = article.get('title', '')  # fallback to title if no tags/categories\n",
    "\n",
    "        article_embedding = get_embedding_with_cache(combined_text, bedrock_client)\n",
    "        similarities = cosine_similarity([article_embedding], section_embedding_matrix)[0]\n",
    "\n",
    "        best_section_idx = int(np.argmax(similarities))\n",
    "        best_similarity = float(similarities[best_section_idx])\n",
    "        best_section = section_names[best_section_idx]\n",
    "\n",
    "        article_copy = article.copy()\n",
    "        if best_similarity >= threshold:\n",
    "            article_copy['taxonomy_section'] = best_section\n",
    "            article_copy['embedding_similarity'] = best_similarity\n",
    "            article_copy['all_section_similarities'] = {\n",
    "                section_names[j]: float(similarities[j]) for j in range(len(section_names))\n",
    "            }\n",
    "        else:\n",
    "            article_copy['taxonomy_section'] = 'Unassigned'\n",
    "            article_copy['embedding_similarity'] = best_similarity\n",
    "            article_copy['all_section_similarities'] = {\n",
    "                section_names[j]: float(similarities[j]) for j in range(len(section_names))\n",
    "            }\n",
    "        updated_articles.append(article_copy)\n",
    "    return updated_articles\n",
    "\n",
    "# Example usage:\n",
    "taxonomy = load_taxonomy(\"taxonomy_results_1.json\")\n",
    "mapped_articles = map_articles_to_taxonomy_by_tags_categories(\n",
    "    articles, taxonomy, bedrock_client, threshold=0.7\n",
    ")\n",
    "print(f\"✓ Mapped {len(mapped_articles)} articles to taxonomy sections using tags/categories embeddings\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results and their taxonomy sections\n",
    "  # Print first 5 mapped articles for review\n",
    "for article in mapped_articles[:5]:\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"  Assigned Section: {article['taxonomy_section']} (similarity: {article['embedding_similarity']:.3f})\")    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced mapping: average embeddings for tags, categories, summary, and content, then compare to taxonomy section embeddings\n",
    "\n",
    "def get_average_embedding(terms: List[str], bedrock_client) -> np.ndarray:\n",
    "    \"\"\"Generate and average embeddings for a list of terms.\"\"\"\n",
    "    embeddings = []\n",
    "    for term in terms:\n",
    "        if term and isinstance(term, str) and term.strip():\n",
    "            emb = get_embedding_with_cache(term.strip(), bedrock_client)\n",
    "            embeddings.append(emb)\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        # Return zero vector if no valid terms\n",
    "        return np.zeros(1536)\n",
    "\n",
    "def get_article_composite_embedding(article: Dict[str, Any], bedrock_client, weights=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a composite embedding for an article using tags, categories, summary, and content.\n",
    "    weights: dict with keys 'tags', 'categories', 'summary', 'content'\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {'tags': 0.2, 'categories': 0.2, 'summary': 0.3, 'content': 0.3}\n",
    "    emb_tags = get_average_embedding(article.get('llm_tags', []), bedrock_client)\n",
    "    emb_categories = get_average_embedding(article.get('llm_categories', []), bedrock_client)\n",
    "    emb_summary = get_average_embedding([article.get('llm_summary', '')], bedrock_client)\n",
    "    emb_content = get_average_embedding([article.get('content', '')], bedrock_client)\n",
    "    # Weighted average\n",
    "    composite = (\n",
    "        weights['tags'] * emb_tags +\n",
    "        weights['categories'] * emb_categories +\n",
    "        weights['summary'] * emb_summary +\n",
    "        weights['content'] * emb_content\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "def map_articles_to_taxonomy_by_composite_embeddings(\n",
    "    articles: List[Dict[str, Any]],\n",
    "    taxonomy: Dict[str, Any],\n",
    "    bedrock_client,\n",
    "    threshold: float = 0.7,\n",
    "    weights=None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Map articles to taxonomy sections using composite embeddings (tags, categories, summary, content).\n",
    "    \"\"\"\n",
    "    # Prepare taxonomy section embeddings\n",
    "    section_embeddings = {}\n",
    "    section_names = []\n",
    "    for section in taxonomy['main_sections']:\n",
    "        section_name = section['name']\n",
    "        section_text = f\"{section_name}. {section['description']}. Tags: {', '.join(section['tags'])}\"\n",
    "        section_embedding = get_embedding_with_cache(section_text, bedrock_client)\n",
    "        section_embeddings[section_name] = section_embedding\n",
    "        section_names.append(section_name)\n",
    "    section_embedding_matrix = np.array([section_embeddings[name] for name in section_names])\n",
    "\n",
    "    updated_articles = []\n",
    "    for article in articles:\n",
    "        composite_embedding = get_article_composite_embedding(article, bedrock_client, weights)\n",
    "        similarities = cosine_similarity([composite_embedding], section_embedding_matrix)[0]\n",
    "        best_section_idx = int(np.argmax(similarities))\n",
    "        best_similarity = float(similarities[best_section_idx])\n",
    "        best_section = section_names[best_section_idx]\n",
    "        article_copy = article.copy()\n",
    "        article_copy['composite_embedding_similarity'] = best_similarity\n",
    "        article_copy['composite_best_section'] = best_section\n",
    "        article_copy['composite_all_section_similarities'] = {\n",
    "            section_names[j]: float(similarities[j]) for j in range(len(section_names))\n",
    "        }\n",
    "        # Assign section if above threshold\n",
    "        if best_similarity >= threshold:\n",
    "            article_copy['taxonomy_section'] = best_section\n",
    "        else:\n",
    "            article_copy['taxonomy_section'] = 'Unassigned'\n",
    "        updated_articles.append(article_copy)\n",
    "    return updated_articles\n",
    "\n",
    "# Example usage:\n",
    "taxonomy = load_taxonomy(\"taxonomy_results_1.json\")\n",
    "mapped_articles_composite = map_articles_to_taxonomy_by_composite_embeddings(\n",
    "    articles, taxonomy, bedrock_client, threshold=0.7\n",
    ")\n",
    "print(f\"✓ Mapped {len(mapped_articles_composite)} articles using composite embeddings (tags, categories, summary, content)\")\n",
    "\n",
    "# Show similarity results for first 5 articles\n",
    "for article in mapped_articles_composite[:5]:\n",
    "    print(f\"\\nTitle: {article['title']}\")\n",
    "    print(f\"  Assigned Section: {article['taxonomy_section']} (composite similarity: {article['composite_embedding_similarity']:.3f})\")\n",
    "    print(\"  Section similarities:\")\n",
    "    for sec, sim in article['composite_all_section_similarities'].items():\n",
    "        print(f\"    {sec}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b31981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare article-to-section similarity for each embedding type: text, tags, categories\n",
    "\n",
    "def get_embedding_for_terms(terms: List[str], bedrock_client) -> np.ndarray:\n",
    "    \"\"\"Average embedding for a list of terms (tags or categories).\"\"\"\n",
    "    embeddings = []\n",
    "    for term in terms:\n",
    "        if term and isinstance(term, str) and term.strip():\n",
    "            emb = get_embedding_with_cache(term.strip(), bedrock_client)\n",
    "            embeddings.append(emb)\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(1536)\n",
    "\n",
    "def get_embedding_for_text(text: str, bedrock_client) -> np.ndarray:\n",
    "    \"\"\"Embedding for full text.\"\"\"\n",
    "    if text and isinstance(text, str) and text.strip():\n",
    "        return get_embedding_with_cache(text.strip(), bedrock_client)\n",
    "    else:\n",
    "        return np.zeros(1536)\n",
    "\n",
    "def compare_article_to_taxonomy_sections(article, taxonomy, bedrock_client):\n",
    "    \"\"\"Return similarity scores for each section for text, tags, categories.\"\"\"\n",
    "    # Prepare section embeddings\n",
    "    section_embeddings = {}\n",
    "    section_names = []\n",
    "    for section in taxonomy['main_sections']:\n",
    "        section_name = section['name']\n",
    "        section_text = f\"{section_name}. {section.get('description', '')}. Tags: {', '.join(section.get('tags', []))}\"\n",
    "        section_embedding = get_embedding_with_cache(section_text, bedrock_client)\n",
    "        section_embeddings[section_name] = section_embedding\n",
    "        section_names.append(section_name)\n",
    "    section_embedding_matrix = np.array([section_embeddings[name] for name in section_names])\n",
    "\n",
    "    # Article embeddings\n",
    "    text_emb = get_embedding_for_text(article.get('full_text', ''), bedrock_client)\n",
    "    tags_emb = get_embedding_for_terms(article.get('llm_tags', []), bedrock_client)\n",
    "    cats_emb = get_embedding_for_terms(article.get('llm_categories', []), bedrock_client)\n",
    "\n",
    "    # Similarities\n",
    "    sim_text = cosine_similarity([text_emb], section_embedding_matrix)[0]\n",
    "    sim_tags = cosine_similarity([tags_emb], section_embedding_matrix)[0]\n",
    "    sim_cats = cosine_similarity([cats_emb], section_embedding_matrix)[0]\n",
    "\n",
    "    # Results\n",
    "    results = {\n",
    "        'title': article.get('title', ''),\n",
    "        'similarities': {\n",
    "            'text': dict(zip(section_names, sim_text)),\n",
    "            'tags': dict(zip(section_names, sim_tags)),\n",
    "            'categories': dict(zip(section_names, sim_cats)),\n",
    "        }\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Example: Show similarity scores for first 3 articles\n",
    "taxonomy = load_taxonomy(\"taxonomy_results_1.json\")\n",
    "for article in articles[:3]:\n",
    "    res = compare_article_to_taxonomy_sections(article, taxonomy, bedrock_client)\n",
    "    print(f\"\\nTitle: {res['title']}\")\n",
    "    print(\"  Text embedding similarity:\")\n",
    "    for sec, sim in res['similarities']['text'].items():\n",
    "        print(f\"    {sec}: {sim:.3f}\")\n",
    "    print(\"  Tags embedding similarity:\")\n",
    "    for sec, sim in res['similarities']['tags'].items():\n",
    "        print(f\"    {sec}: {sim:.3f}\")\n",
    "    print(\"  Categories embedding similarity:\")\n",
    "    for sec, sim in res['similarities']['categories'].items():\n",
    "        print(f\"    {sec}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786611f",
   "metadata": {},
   "source": [
    "## export embeddings for vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_umap_to_json(embedding_2d, items, output_file=\"umap_data.json\"):\n",
    "  \"\"\"\n",
    "  Export UMAP 2D coordinates and metadata to a JSON file for web visualization.\n",
    "  embedding_2d: np.ndarray of shape (n_items, 2)\n",
    "  items: list of dicts with metadata for each item (articles, tags, sections, etc.)\n",
    "  output_file: path to save JSON\n",
    "  \"\"\"\n",
    "  export_data = []\n",
    "  for i, item in enumerate(items):\n",
    "    export_data.append({\n",
    "      \"x\": float(embedding_2d[i, 0]),\n",
    "      \"y\": float(embedding_2d[i, 1]),\n",
    "      \"label\": item.get(\"title\") or item.get(\"name\"),\n",
    "      \"type\": item.get(\"type\"),\n",
    "      \"cluster\": item.get(\"cluster\"),\n",
    "      \"metadata\": item\n",
    "    })\n",
    "  with open(output_file, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "  print(f\"✓ Exported UMAP data to {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "export_umap_to_json(embedding_2d, items, \"umap_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a9e2",
   "metadata": {},
   "source": [
    "## Using taxonomy with LLM to classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc214909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Using the Taxonomy - not sure about this one\n",
    "\n",
    "def classify_new_article(article_content: str, article_title: str, \n",
    "                        taxonomy: Dict[str, Any], \n",
    "                        bedrock_client) -> Dict[str, Any]:\n",
    "    \"\"\"Classify a new article using the existing taxonomy.\"\"\"\n",
    "    print(f\"Classifying new article: '{article_title}'...\")\n",
    "    \n",
    "    # Get the consolidated taxonomy\n",
    "\n",
    "    main_sections = taxonomy['main_sections']\n",
    "    all_tags = taxonomy['all_tags']\n",
    "    \n",
    "    # Prepare section descriptions for the LLM\n",
    "    section_descriptions = []\n",
    "    for section in main_sections:\n",
    "        section_desc = f\"- {section['name']}: {section['description']} (Tags: {', '.join(section['tags'])})\"\n",
    "        section_descriptions.append(section_desc)\n",
    "    \n",
    "    # Create classification prompt\n",
    "    prompt = f\"\"\"\n",
    "        Given the following blog taxonomy and a new article, classify the article:\n",
    "\n",
    "        BLOG TAXONOMY:\n",
    "        {chr(10).join(section_descriptions)}\n",
    "\n",
    "        AVAILABLE TAGS: {', '.join(all_tags)}\n",
    "\n",
    "        NEW ARTICLE:\n",
    "        Title: {article_title}\n",
    "        Content: {article_content[:3000]}...\n",
    "\n",
    "        Please classify this article by:\n",
    "        1. Selecting the most appropriate main section(s) from the taxonomy\n",
    "        2. Assigning relevant tags from the available tags list\n",
    "        3. Suggesting any new tags if the existing ones don't fit well\n",
    "\n",
    "        Respond in this exact JSON format:\n",
    "        {{\n",
    "            \"main_sections\": [\"Section Name 1\"],\n",
    "            \"suggested_new_tags\": [\"new_tag1\", \"new_tag2\"],\n",
    "            \"confidence\": \"high/medium/low\",\n",
    "            \"reasoning\": \"Brief explanation of classification decisions (max 2 sentences)\"\n",
    "        }}\"\"\"\n",
    "\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 400,\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=MODEL_SONNET_35,\n",
    "            body=body,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        llm_response = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        classification = json.loads(llm_response)\n",
    "        \n",
    "        print(f\"✓ Article classified successfully\")\n",
    "        print(f\"  Sections: {', '.join(classification['main_sections'])}\")\n",
    "        print(f\"  Tags: {', '.join(classification['existing_tags'])}\")\n",
    "        if classification.get('suggested_new_tags'):\n",
    "            print(f\"  New tags suggested: {', '.join(classification['suggested_new_tags'])}\")\n",
    "        print(f\"  Confidence: {classification['confidence']}\")\n",
    "        \n",
    "        return classification\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error classifying article: {e}\")\n",
    "        return {\n",
    "            \"main_sections\": [],\n",
    "            \"suggested_new_tags\": [],\n",
    "            \"confidence\": \"low\",\n",
    "            \"reasoning\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def create_frontmatter_suggestions(classification: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate frontmatter suggestions for a new article based on classification.\"\"\"\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "        date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "        llm_tags: {classification.get('suggested_new_tags', [])}\n",
    "        llm_categories: {classification['main_sections']}\n",
    "        # Confidence: {classification['confidence']}\n",
    "        # Reasoning: {classification['reasoning']}\n",
    "        ---\"\"\"\n",
    "    \n",
    "    return frontmatter\n",
    "\n",
    "def update_taxonomy_with_new_tags(taxonomy_results: Dict[str, Any], \n",
    "                                 new_tags: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Update taxonomy with new tags discovered from new articles.\"\"\"\n",
    "    if not new_tags:\n",
    "        return taxonomy_results\n",
    "    \n",
    "    print(f\"Adding {len(new_tags)} new tags to taxonomy: {new_tags}\")\n",
    "    \n",
    "    # Add to the consolidated tags list\n",
    "    current_tags = set(taxonomy_results['taxonomy']['all_tags'])\n",
    "    updated_tags = list(current_tags.union(set(new_tags)))\n",
    "    taxonomy_results['taxonomy']['all_tags'] = sorted(updated_tags)\n",
    "    \n",
    "    return taxonomy_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_new_article_with_section_analysis(article_content: str, \n",
    "                                             article_title: str,\n",
    "                                             taxonomy_results: Dict[str, Any],\n",
    "                                             bedrock_client) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced classification that can suggest new sections.\"\"\"\n",
    "    \n",
    "    # Get basic classification first\n",
    "    classification = classify_new_article(article_content, article_title, taxonomy_results, bedrock_client)\n",
    "    \n",
    "    # If confidence is low, ask LLM if a new section might be needed\n",
    "    if classification['confidence'] == 'low':\n",
    "        current_sections = [s['name'] for s in taxonomy_results['taxonomy']['main_sections']]\n",
    "        \n",
    "        section_analysis_prompt = f\"\"\"This article doesn't fit well into existing blog sections: {', '.join(current_sections)}\n",
    "\n",
    "Article: {article_title}\n",
    "Content preview: {article_content[:1000]}...\n",
    "\n",
    "Should a new blog section be created for this type of content? If so, suggest:\n",
    "1. Section name\n",
    "2. Section description  \n",
    "3. What other content might fit in this section\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"suggest_new_section\": true/false,\n",
    "  \"section_name\": \"Proposed Section Name\",\n",
    "  \"section_description\": \"What this section would cover\",\n",
    "  \"rationale\": \"Why this section is needed\"\n",
    "}}\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call LLM for section analysis\n",
    "            body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 300,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": section_analysis_prompt}]\n",
    "            })\n",
    "            \n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                body=body,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            section_analysis = json.loads(response_body['content'][0]['text'].strip())\n",
    "            \n",
    "            # Add section suggestion to classification\n",
    "            classification['section_suggestion'] = section_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            classification['section_suggestion'] = {\"suggest_new_section\": False, \"error\": str(e)}\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2940e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track classifications over time\n",
    "recent_classifications = []\n",
    "\n",
    "file_path = '../src/content/drafts/on-games.md'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "    # Parse frontmatter\n",
    "    post = frontmatter.loads(content)\n",
    "    \n",
    "    # Create full text for embedding (title + content)\n",
    "    title = post.metadata.get(\"title\", \"\")\n",
    "    full_text = f\"{title}\\n\\n{post.content.strip()}\"\n",
    "    \n",
    "    article = {\n",
    "        \"file_path\": str(file_path),\n",
    "        \"filename\": file_path,\n",
    "        \"title\": title,\n",
    "        \"date\": post.metadata.get(\"pubDate\", \"\"),\n",
    "        \"tags\": post.metadata.get(\"tags\", []),\n",
    "        \"metadata\": post.metadata,\n",
    "        \"content\": post.content.strip(),\n",
    "        \"full_text\": full_text,\n",
    "        \"processed_content\": preprocess_text(full_text)\n",
    "    }\n",
    "\n",
    "# For each new article\n",
    "new_classification = classify_new_article_with_section_analysis(\n",
    "    article ['processed_content'], article['title'], taxonomy_results, bedrock_client\n",
    ")\n",
    "\n",
    "recent_classifications.append(new_classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf026dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Periodically analyze if taxonomy needs expansion\n",
    "if len(recent_classifications) >= 1:  # Every 10 articles\n",
    "    evolution_analysis = analyze_taxonomy_evolution(\n",
    "        taxonomy_results, recent_classifications, bedrock_client\n",
    "    )\n",
    "    \n",
    "    if evolution_analysis['needs_new_sections']:\n",
    "        print(\"🚨 TAXONOMY UPDATE RECOMMENDED:\")\n",
    "        for suggestion in evolution_analysis['suggested_sections']:\n",
    "            print(f\"• New Section: {suggestion['name']}\")\n",
    "            print(f\"  Description: {suggestion['description']}\")\n",
    "            print(f\"  Rationale: {suggestion['rationale']}\")\n",
    "            print(f\"  Tags: {', '.join(suggestion['suggested_tags'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Enhanced Results with LLM Analysis\n",
    "def create_enhanced_results_df(articles: List[Dict[str, Any]], categories: Dict[int, str]) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive DataFrame with all analysis results.\"\"\"\n",
    "    \n",
    "    # Create base DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Add cluster categories\n",
    "    df['cluster_category'] = df['cluster'].map(lambda x: categories.get(x, 'Unclustered'))\n",
    "    \n",
    "    # Add LLM analysis columns if available\n",
    "    if 'llm_tags' in df.columns:\n",
    "        df['llm_tags_str'] = df['llm_tags'].apply(lambda x: ', '.join(x) if x else '')\n",
    "        df['llm_main_points_str'] = df['llm_main_points'].apply(\n",
    "            lambda x: ' | '.join(x) if x else ''\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create enhanced results DataFrame\n",
    "enhanced_df = create_enhanced_results_df(articles, categories)\n",
    "\n",
    "print(f\"\\n💾 Enhanced results saved to 'enhanced_df' DataFrame\")\n",
    "print(\"   New columns: llm_tags, llm_main_points, llm_summary, cluster_category\")\n",
    "\n",
    "# Show comparison of original tags vs LLM tags\n",
    "if 'llm_tags' in enhanced_df.columns:\n",
    "    print(f\"\\n🔍 TAG COMPARISON (Original vs LLM):\")\n",
    "    for _, row in enhanced_df.head(3).iterrows():\n",
    "        print(f\"  {row['title'][:40]}...\")\n",
    "        print(f\"    Original tags: {row['tags']}\")\n",
    "        print(f\"    LLM tags: {row['llm_tags']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in analyzed_articles:\n",
    "    print(f\"\\n📄 {article['title']}\")\n",
    "    print(f\"   🏷️  LLM Tags: {', '.join(article['llm_tags'])}\")\n",
    "    print(f\"   📋 Main Points:\")\n",
    "    for point in article['llm_main_points']:\n",
    "        print(f\"      • {point}\")\n",
    "    print(f\"   📝 Summary: {article['llm_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21c45c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
